<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kristin Branson, with ideas and edits from Diane Adjavon, John Bogovic, and Janelia CVML attendees">
<meta name="dcterms.date" content="2025-02-14">
<meta name="description" content="Summary of and thoughts on Meta’s recent paper">

<title>Large Concept Models: Language Modeling in a Sentence Representation Space – Janelia CVML</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Large Concept Models: Language Modeling in a Sentence Representation Space – Janelia CVML">
<meta property="og:description" content="Summary of and thoughts on Meta’s recent paper">
<meta property="og:image" content="https://janelia-cvml.github.io/blog/posts/LargeConceptModels/LCMArchitecture.png">
<meta property="og:site_name" content="Janelia CVML">
<meta property="og:image:height" content="455">
<meta property="og:image:width" content="488">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../cvml_300x110.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Janelia CVML</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.janelia.org/our-research/computation-and-theory"> <i class="bi bi-card-text" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Large Concept Models: Language Modeling in a Sentence Representation Space</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source" data-quarto-source-url="https://github.com/janelia-cvml/blog/blob/main/posts/LargeConceptModels/index.qmd"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          Summary of and thoughts on Meta’s recent paper
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Transformers</div>
                <div class="quarto-category">Multi Scale</div>
                <div class="quarto-category">Language</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Kristin Branson, with ideas and edits from Diane Adjavon, John Bogovic, and Janelia CVML attendees </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 14, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><em><a href="https://arxiv.org/abs/2412.08821">Large Concept Models</a> was discussed in Janelia CVML 2025-02-07.</em></p>
<div id="fig-lcm-architecture" class="quarto-float quarto-figure quarto-figure-center anchored" style="width: 300px; float: left; margin-right: 1em;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lcm-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="LCMArchitecture.png" class="img-fluid figure-img" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lcm-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Large Concept Model (LCM) architecture <a href="#LCMTeam2024">[LCM Team, 2024]</a>
</figcaption>
</figure>
</div>
<p>I’ve been interested in what <strong>temporal scales</strong> it makes sense to model time series data at. Nearly all language modeling approaches represent text at the word (token) level. There is no explicit representation of the concepts underlying text at larger scales, while it seems like our own internal representations of language exist at multiple scales. Would explicitly representing larger scales improve how well language models capture semantic concepts, model longer-range temporal relationships, even reason? Or does word-token-level modeling implicitly capture this structure?</p>
<p><a href="https://arxiv.org/abs/2412.08821">Large Concept Models</a> is an interesting attempt at explicitly modeling structure at a larger scale: <strong>sentences</strong>. Like spaces between words, punctuation between sentences provides a non-arbitrary way to segment text at a scale ~50 times that of word tokens. This work posits that this <strong>sentence-scale representation corresponds to concepts</strong>, hence the paper title. The basic idea implemented is straightforward (<a href="#fig-lcm-architecture" class="quarto-xref">Figure&nbsp;1</a>):</p>
<ul>
<li>Use an existing model (<a href="#SONAR">SONAR</a>) to <strong>embed sentences</strong> into a “concept” space.</li>
<li>Train a transformer that <strong>predicts sentence embeddings</strong> rather than word tokens.</li>
<li>At inference, convert predicted sentence embeddings to text through the embedding model’s decoder.</li>
</ul>
<section id="concepts-and-resolutions" class="level2">
<h2 class="anchored" data-anchor-id="concepts-and-resolutions">Concepts and resolutions</h2>
<div id="fig-sentence-length" class="quarto-float quarto-figure quarto-figure-center anchored" style="width: 300px; float: right; margin-left: 1em;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sentence-length-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="SentenceSize.png" class="img-fluid figure-img" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sentence-length-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Sentence length vs embed-ability <a href="#LCMTeam2024">[LCM Team, 2024]</a>
</figcaption>
</figure>
</div>
<p>One question about this approach is the <strong>choice of resolutions</strong>. Does a sentence really correspond to a concept? Or should we rather use parts of speech? Paragraphs? Or should segmentation be something determined based on the text? Should language be modeled at many resolutions at a time? One of the details of the paper was that dividing text into sentences was non-trivial, and the method improved by capping the maximum sentence length at 200 characters, suggesting that the next level of resolution after words perhaps should be shorter than a sentence (<a href="#fig-sentence-length" class="quarto-xref">Figure&nbsp;2</a>).</p>
<p>This choice could be because the sentence embedding model, SONAR <a href="#SONAR">[Duquenne, 2023]</a> was “trained on … bitext machine translation data containing rather short sentences.” Since its training was targeting machine translation, it is unclear whether it encodes “concepts” or some other semantics of sentences. Future work could explore different choices for the sentence embedding, perhaps seeking to more directly encode “concepts” from sentences, or by training the encoder end-to-end, challenging though that may be.</p>
<p>Another question is about the <strong>integration of resolutions</strong>. The model trained here really operated only at the sentence-resolution, with the word encoder/decoder being done completely independently. Not only are the encoder and decoder fixed (as shown in <a href="#fig-lcm-architecture" class="quarto-xref">Figure&nbsp;1</a>), words are not <em>seen or produced</em> by the LCM – both training and inference are done solely at the sentence level. Thus, this model is really a single-resolution model that operates at a larger scale than standard LLMs. This has the benefit of helping greatly with computational complexity, allowing the network to potentially reason over much longer context lengths. However, we suspect that this hurts the quality of the text produced. One caveat to this is that the proposed approaches – the diffusion LCM and quantized LCM – predict the sentence embedding with coarse-to-fine resolution, so perhaps some sub-sentence structure is also represented.</p>
</section>
<section id="transformers-for-high-d-continuous-time-series-data" class="level2">
<h2 class="anchored" data-anchor-id="transformers-for-high-d-continuous-time-series-data">Transformers for high-d, continuous time-series data</h2>
<p>One of the strengths of this paper are the many approaches tried for adapting transformers to model sentence embeddings, which live in a 1024-dimensional <strong>continuous</strong> space. The authors try three methods:</p>
<ul>
<li>The <strong>baseline</strong> LCM predicts the continuous embedding vector with a MSE loss – it does not try to model the possibly multi-modal next-sentence distribution.</li>
<li>The <strong>diffusion</strong>-based LCMs use diffusion models to model this continuous distribution.</li>
<li>The <strong>quantized</strong> LCMs first quantize the sentence embedding space, then predict</li>
</ul>
<section id="diffusion-lcms---details" class="level3">
<h3 class="anchored" data-anchor-id="diffusion-lcms---details">Diffusion LCMs - details</h3>
<div id="fig-diffusion-lcm" class="quarto-float quarto-figure quarto-figure-center anchored" style="width: 200px; float: left; margin-right: 1em;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diffusion-lcm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="DiffusionAttentionMasks.png" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diffusion-lcm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Attention mask for training diffusion LCMs <a href="#LCMTeam2024">[LCM Team, 2024]</a>
</figcaption>
</figure>
</div>
<p>Let <span class="math inline">\(x_n\)</span> be the <span class="math inline">\(n\)</span>th sentence embedding and <span class="math inline">\(x_n^t\)</span> be the embedding after <span class="math inline">\(t\)</span> diffusion steps. This approach models the noiseless <span class="math inline">\(n\)</span>th sentence embedding <span class="math inline">\(x_n^0\)</span> given a noisy version <span class="math inline">\(x_n^t\)</span> and the previous noiseless sentences <span class="math inline">\(x_{&lt;n}^0\)</span>: <span class="math inline">\(p_\theta(x_n^0 | x_n^t, t, x_{&lt;n}^0)\)</span>. This can be set up to train efficiently with attention masks, allowing all sentences in a document to be predicted simultaneously. There are a <strong>lot of details</strong> about all the diffusion model tricks they used, including the noise schedule, sample weighting, and classifier-free diffusion guidance. Their <a href="https://upload.wikimedia.org/wikipedia/en/c/cc/Tolkien%27s_design_for_The_Two_Towers_cover.jpg">two-tower</a> diffusion LCM separates the encoding of previous sentences <span class="math inline">\(x_{&lt;n}^0\)</span> from the denoising process from <span class="math inline">\(x_n^t\)</span> with one transformer tower each. These details are a great resource for someone trying to train transformer-diffusion models for time series data.</p>
</section>
<section id="quantized-lcms---details" class="level3">
<h3 class="anchored" data-anchor-id="quantized-lcms---details">Quantized LCMs - details</h3>
<div id="fig-quantized-lcm" class="quarto-float quarto-figure quarto-figure-center anchored" style="width: 200px; float: right; margin-left: 1em;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quantized-lcm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="QuantizedLCM.png" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantized-lcm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Quantized LCM training architecture (my guess from text)
</figcaption>
</figure>
</div>
<p>Quantized LCMs first tokenize the sentence embedding space, which makes the LCM mechanically more like a standard LLM. Specifically, they use the <a href="#RVQ">Residual Vector Quantization</a> which converts a sentence embedding into a sequence of 64 coarse-to-fine codewords. My guess of how this was done based on the details in the text is as follows. Let <span class="math inline">\(c_n^t\)</span> be the <span class="math inline">\(t\)</span>th resolution codeword for sentence <span class="math inline">\(n\)</span>, and let <span class="math inline">\(\mu^t_c\)</span> be the <span class="math inline">\(c\)</span>th codeword for the <span class="math inline">\(t\)</span>th resolution codebook. Then we can construct the <span class="math inline">\(t\)</span>th resolution approximation of our sentence embedding from codewords <span class="math inline">\((c_{T-1},...,c_0)\)</span> as <span class="math display">\[x_n^t = \sum_{i\geq t} \mu^i_{c_n^i}\]</span> Then, they can do something similar to the one-tower diffusion LCM, replacing <span class="math inline">\(\hat{x}^0_n\)</span> with <span class="math inline">\(c^{t_n+1}_n\)</span> (<a href="#fig-quantized-lcm" class="quarto-xref">Figure&nbsp;4</a>).</p>
<p>They also tried an approach, termed QUANT-LCM-C, where they predict the continuous target embedding from the intermediate quantized representation, and at inference time, they sample a codeword based on the distance to the predicted residual. Depending on details here, I’m not sure if, during training, the distribution is modeled, or if it is just a way to add some noise at inference time.</p>
</section>
<section id="coarse-to-fine-predictions" class="level3">
<h3 class="anchored" data-anchor-id="coarse-to-fine-predictions">Coarse-to-fine predictions</h3>
<p>A notable commonality between the diffusion LCM and quantized LCM is the <strong>coarse-to-fine</strong> resolution (<span class="math inline">\(t = T, ..., 0\)</span>) for prediction. One question I have is whether this allows more resolutions of language between word and sentence to be represented. While these intermediate scales are part of the prediction, I note that they are not part of the left-context the transformer has access to; they are only used for the current sentence. Still, this could be an interesting avenue to explore in the future – what if they were not so clever with their attention masks (<a href="#fig-diffusion-lcm" class="quarto-xref">Figure&nbsp;3</a>, <a href="#fig-quantized-lcm" class="quarto-xref">Figure&nbsp;4</a>), and allowed intermediate representations in the context? Would this be an avenue to a more multi-resolution language model?</p>
</section>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<p>The paper includes an <strong>extensive description</strong> of experiments conducted to compare these different LCM models to each other and to word-based LLMs. One of the difficulties with language modeling in general is that there is not a single way to measure which model performs best, and the results comparing LCMs, to me, appear to be <strong>conflicting</strong>, with the baseline model performing best in terms of simple <span class="math inline">\(l_2\)</span> distance, and diffusion models performing better for some more complex metrics while quantization methods perform better for others.</p>
<p>The authors conclude that “diffusion-based methods give clearly better results compared to all other models”, and I couldn’t tell what this was based on, from the tables of results provided. We noted that there isn’t a single example output in the paper, and think a good rule of thumb with ML experimental sections is you shouldn’t <em>only</em> have example outputs, but you also shouldn’t <em>only</em> have tables of bolded numbers.</p>
<p>One reason I could imagine the quantized LCM performing worse is because of the choice of the fixed embedding space. The sentence embedding to words decoder is likely not robust to deviations from the sub-space of the 1024-dimensional ambient embedding space that it was trained on, and the diffusion models are likely better than the quantized models at producing examples in that sub-space. By analogy, I’m thinking of how GANs produce more realistic images than VAEs but VAEs may cover the full distribution better, and I think this may be true for diffusion models vs quantized models. Perhaps a text example produced by a diffusion model is harder to distinguish from human text, but fails at capturing the full distribution. Would that be captured by any of these metrics? The authors suggest that training the embedding space to be quantize-able may improve this method.</p>
<div id="fig-summarization-results" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" style="text-align: center;">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-summarization-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="SummarizationResults.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-summarization-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: [<a href="#LCMTeam2024">LCM Team, 2024</a>]
</figcaption>
</figure>
</div>
<p>They also compare to standard, word-token-based LLMs on summarization tasks, again with conflicting results, depending on the metric. This is the disappointing part of the paper – there’s not really an obvious (to me) benefit of the sentence-resolution modeling over word-resolution modeling. Does that mean larger-scale resolutions are implicitly learned in word-resolution models, or that there is still work to do to get explicit larger-scale resolution models to work? As the authors conclude, “[t]he choice and design of the embedding space plays a crucial role”, and note differences in the data used to train the embedding space and their LCM, and potential issues with the frozen encoder.</p>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing thoughts</h2>
<p>The paper presents an interesting idea in a direction I think is promising – explicit representation of longer time scales in time-series-data modeling. I’m happy to see people working on it! It also has some creative ideas for how to use transformers to predict continuous, high-dimensional time-series data, which I plan to try out on my own data. I appreciate the detail the authors provided on many of their methods, including the tweaks they tried, even when they didn’t improve results. It was clearly a tremendous amount of work.</p>
<p>Ultimately, my guess from the experiments is that this method did not, on its own, improve the quality of text generated beyond what word-based LLMs produce. I’d guess that in future work, the authors or others will explore:</p>
<ul>
<li>Simultaneously modeling more scales during training and inference,</li>
<li>Learning segmentations rather than imposing them from sentence structure,</li>
<li>Jointly learning concept embeddings with the language model.</li>
</ul>
<p>In my opinion, the question of whether explicitly modeling longer time scales is helpful for language modeling remains open and compelling!</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p><a id="LCMTeam2024">[LCM Team, 2024]</a> LCM team, Loïc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-jussà, David Dale, Hady Elsahar, Kevin Heffernan, João Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo Sánchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, Holger Schwenk, “Large Concept Models: Language Modeling in a Sentence Representation Space”, 2024. <a href="https://arxiv.org/abs/2412.08821"><span class="emoji" data-emoji="link">🔗</span></a>, <a href="https://github.com/facebookresearch/large_concept_model">[code]</a>.</p>
<p><a id="SONAR">[Duquenne, 2023]</a> Paul-Ambroise Duquenne, Holger, Schwenk, and Benoit Sagot, “SONAR: Sentence-Level Multimodal and Language-Agnostic Representations”, 2023. <a href="https://github.com/facebookresearch/SONAR"><span class="emoji" data-emoji="link">🔗</span></a></p>
<p><a id="RVQ">[Hujben, 2024]</a> Iris A. M. Huijben, Matthijs Douze, Matthew Muckley, Ruud J. G. van Sloun, Jakob Verbeek, “Residual Quantization with Implicit Neural Codebooks”, 2024 <a href="https://arxiv.org/abs/2401.14732"><span class="emoji" data-emoji="link">🔗</span></a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/janelia-cvml\.github\.io\/blog");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>