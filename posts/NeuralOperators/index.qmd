---
title: "Neural Operators"
from: markdown+emoji
description: "An Introduction to Neural Networks on Continuous Domains"
author: "Magdalena Schneider and Michael Innerberger"
date: "2024-11-29"
bibliography: references.bib
categories: [AIForScience, Partial Differential Equations, Numerical Methods, NeuralOperators]
image: "no-architecture.png"  # Optional
---

*Neural Operators [@Kovachki2021] and Fourier Neural Operator [@Li2021] papers were presented by Michael Innerberger and Magdalena Schneider in Janelia CVML 2024-11-01 (and discussed in this blog post), Adaptive Fourier Neural Operator [@Guibas2022] and FourCastNet [@Pathak2022] papers were presented by Magdalena Schneider and Kristin Branson in Janelia CVML 2024-11-08 (see blog post on [FourCastNet](../FourCastNet/index.qmd))*

Neural Operators are a generalization of neural networks to continuous domains, i.e., they learn to map functions to other functions.
A major advantage of neural operators is their (presumed) discretization invariance, which allows for zero-shot super resolution, meaning that the output function can be evaluated at an arbitrary set of query points without the need to retrain the neural operator.
Originally developed for solving PDEs, neural operators have been successfully applied to a wide range of problems, including computer vision tasks.
In this blog post, we will introduce the concept of neural operators, discuss different ways to represent the kernel, and present some hands-on examples.

We assume that you are familiar with the very basics of Partial Differential Equations (PDEs) and fundamental solutions to PDEs as this is the theoretical foundation of Neural Operators.


## What are Neural Operators?

Conventional neural networks have finite-dimensional inputs and outputs, which are represented as vectors.
However, many real-world problems are continuous in nature and better represented by a function.
Neural Operators are a generalization of neural networks that can learn to map functions to functions.
They can be used to solve PDEs, inverse problems, and other tasks that involve finding continuous functions.
Neural operators can also be applied to learn mappings between neural representations, as neural representations are function representations.

::: {#fig-neural-operator style="width: 700px; margin: auto; text-align: center;"}
![](neural-operator.png){width=700px}

Principle of neural operators: An input function $a$ is mapped to an output function $u$.
:::

### Motivation
The main motivation behind Neural Operators is that some linear PDEs of the form $\mathsf{L}_a u = f$ with parameter functions $a \colon D \to \mathbb{R}^m$ in some domain $D \subset \mathbb{R}^d$ and source term $f$ can be solved by convolution with a fundamental solution $k$ of the PDE.
Note that either the source term $f$ or the parameter function $a$ can be fixed, and the other is the input to the solution operator which gives us $u$.
If we want to find the solution for varying $f$, the solution operator $G \colon f \mapsto u$ mapping a given source term $f$ to the solution $u$ is given by
$$
u(x) = \int_D k(x, y) f(y) \, dy.
$$ {#eq-fundamental-solution}
While this is only possible for a small set of very specific PDEs, this representation of the solution serves as a basic building block for neural operators, on top of which a deep learning architecture is built.
We stress that @eq-fundamental-solution is only a motivation and not a theoretical justification for why neural operators work.
In particular, neural operators can be applied to a much wider range of problems such as non-linear PDEs and inverse problems; in particular, in the following, we consider the case where the source term $f$ is fixed and the Neural Operator learns to map a parameter function $a$ to the solution $u$.


### General architecture
Put simply, Neural Operators can be seen as a generalization of convolutional neural networks (CNNs) to continuous domains.
While CNNs feature convolutions with small kernels on regular discrete grids, neural operators have a continuous kernel that is convolved with the whole function in each layer.
The general architecture of a Neural Operator is shown in @fig-no-architecture.
The input function $a$ (usually represented by a set of query points and corresponding function values---see @sec-discretization-invariance) is lifted into a (usually higher-dimensional) space by $P$.
Then, the lifted function is processed by a series of layers, each of which applies a convolution with a continuous kernel to the function.
The resulting function is added to a linear term $W v$ and then passed through a non-linear activation function; note that this is akin to a skip connection in a CNN.
Finally, the result of this computation is projected to the output space by the projection $Q$, yielding the output function $u$.
Note that the output function can be queried at any point in the domain, which is a key feature of Neural Operators.

::: {#fig-no-architecture style="width: 700px; margin: auto; text-align: center;"}
![](no-architecture.png){width=700px}

General architecture of a Neural Operator.
The input function $a(x)$ is lifted into a (higher-dimensional) space by $P$, processed by several Neural Operator layers, and projected to the output space by $Q,$ yielding the output function $u(x)$.
Figure adapted from [Kovachki, 2021](#Kovachki2021).
:::


### Discretization and invariance{#sec-discretization-invariance}

Although Neural Operators are designed to learn mappings between functions on continuous domains, for numerical implementation, the continuous functions still needs to be discretized, i.e., represented by a set of query points and corresponding function values $\{x_i, a(x_i)\}_i$.
The query points of $u(x)$ are supplied together with the input function, as they need to pass through the whole network.
While, in principle, the query points can be arbitrary, in practice they are often chosen to be equidistantly spaced on a grid for computational efficiency.
Hence, the true learned operator $\widehat{G} \colon a \mapsto u$ (operating on functions) is approximated by the discretized operator $\widehat{G}_L$, operating on
$$
\widehat{G}_L \colon \mathbb{R}^{L \cdot m} \times \mathbb{R}^{L \cdot d} \rightarrow \mathcal{U},
$$
where $L$ is the number of query points of the input function, $d$ and $m$ are the dimensions of the domain $D$ and the image of the input function, respectively, and $\mathcal{U}$ is the function space of the output function.
Notably, the output function $u(x)$ can be evaluated at any set of query points, which do not have to be the same as the query points of the input function, or the query points used for training.
This allows for "zero-shot super resolution", i.e., the output function can be evaluated at arbitrarily high resolution without the need to retrain the neural operator at different resolutions.

Discretization invariance refers to the fact that $\widehat{G}_L$ converges to $\widehat{G}$ as $L \to \infty$ (under some mild conditions to the distribution of the query points).
Again, increasing $L$ does not require retraining the Neural Operator.
Note that some other papers have claimed that Neural Operators are not necessarily discretization invariant (e.g., @Wang2023); this is a topic of ongoing research.


::: {#fig-discretiation-invariance style="width: 500px; margin: auto; text-align: center;"}
![](discretization-invariance.png){width=500px}

Neural Operators are discretization invariant.
The output can be evaluated at an arbitrary set of query points, which do not have to be the same as the query points of the input function, or the query points used for training.
Figure adapted from @Kovachki2021.
:::




## Different ways to represent the kernel

The bottleneck of Neural Operators is the convolution operation, the cost of which scales with the product of input and output size.
Assuming that input and output are queried at the same $L$ points, the complexity of the convolution is $\mathcal{O}(L^2)$, which is infeasible for most applications.
Fortunately, there are multiple ways to speed up the computation of the convolution based on the structure of the kernel, which we will shortly describe in the following (for details see @Kovachki2021).

### Graph Neural Operators

Instead of taking into account the full space of pairs of sample points, we can limit the evaluation of the kernel to pairs of points that are close to each other.
In practice, this can be done by constructing a graph that connects points that are within a certain distance of each other by using a nearest neighbor search.
It is then advantageous to use graph neural networks to compute the convolution operation on this graph structure, and to be able to backpropagate easily through this operation.
This reduces the complexity to $\mathcal{O}(kL)$, where $k$ is the maximal number of neighbors within a specified distance.
Note that $k$ can, in general, not be chosen independently of $L$ (the number of query points of the input function) as $L$ increases.

::: {#fig-kernel-gno style="width: 400px; margin: auto; text-align: center;"}
![](kernel-gno.png){}

Kernel representation of a GNO (adapted from @Kovachki2021)
:::


### Multipole Graph Neural Operators

Graph Neural Operators only take nearest neighbor interactions into account.
While this might be sufficient for some applications, long-range interactions also play a crucial role in many real-world problems (e.g., celestial mechanics, or electrodynamics).
Drawing from ideas of the fast multipole method (FMM) presented in @FMM, Multipole Graph Neural Operators (MGNOs) use a hierarchical sequence of graphs to approximate the convolution operation.
Every node in one of these graphs summarizes the information of a group of nodes in the previous hierarchy level as shown in @fig-kernel-mgno, and every level of the hierarchy gives rise to a certain scale-dependent component of the kernel.
While the details are complex, the idea is similar to enlarging the context window of a CNN by using a sequence of convolutional layers with small kernel sizes.
This reduces the complexity to $\mathcal{O}(L)$.

::: {#fig-kernel-mgno style="width: 250px; margin: auto; text-align: center;"}
![](kernel-mgno.png){}

Multi-level graph used in MGNOs (adapted from @Kovachki2021)
:::


### Low-rank Neural Operators

By assuming that the kernel can be approximated by a low-rank factorization,
$$k(x, y) \approx \sum_{i=1}^r \varphi_i(x) \varphi_i(y),$$
with learnable functions $\varphi_i \colon D \to \mathbb{R}$, and $r \ll L$, the convolution operation can be computed in $\mathcal{O}(rL)$.
While this can significantly reduce the computational cost, it is important to note that the low-rank assumption might not hold for all problems and that the choice of low-rank approximation (i.e., the functions $\varphi$) can be crucial but non-trivial.

::: {#fig-kernel-lno style="width: 400px; margin: auto; text-align: center;"}
![](kernel-lno.png){}

Kernel representation of a LNO (adapted from @Kovachki2021)
:::


### Fourier Neural Operators

Fourier Neural Operators (FNOs) are based on the Fourier representation of the convolution operation:
$$\int_D k(x, y) v(y) \, dy = \mathcal{F}^{-1}(\mathcal{F}(k) \odot \mathcal{F}(v)),$$
where $\mathcal{F}$ is the Fourier transform, and $\odot$ denotes the element-wise vector or matrix product.
In practice, this seems to give the best results and can be combined with low-pass filtering to reduce the computational cost even further; see @fig-kernel-fno, where the Fourier representation $R$ of the kernel is learned and only low frequencies are propagated through the network.

While, in general, computing the Fourier transform scales quadratically with the input size, the fast Fourier transform (FFT) can be used to compute it in almost linear time if the input grid is uniform on a periodic domain (which by embedding the domain $D$ in a larger domain and extending the input function by zero padding can be achieved for many applications).
In this case, the complexity of the convolution operation is $\mathcal{O}(L \log L)$.

::: {#fig-kernel-fno style="width: 400px; margin: auto; text-align: center;"}
![](kernel-fno.png){}

Kernel representation of a FNO (adapted from @Li2020)
:::


## Practical examples

@Kovachki2021 provides various examples of solving PDEs with Neural Operators.
We picked three examples here that (i) compare the performance of neural operators to the analytic solution, (ii) show a simple example that you can train yourself with code provided by the authors, and (iii) demonstrate how Neural Operators can solve inverse problems much faster than classical methods. 

Note that, in order to generate training data, in @Kovachki2021 the PDEs were solved with classical numerical methods, such as finite differences, finite elements, or spectral methods; see, e.g., @Bartels2018.
Of note, all of these methods require knowledge of the PDE that models the physical phenomenon of interest, and they take significant resources to implement and compute the solution.
While using data from classical methods to train is one common approach, it is also possible to train neural operators directly from data, which was done, for example, in the FourCastNet paper [@Pathak2022], and is an active area of research.

### Poisson equation: compare fundamental solutions
The Poisson equation is a fairly simple PDE that has an analytical solution.
We can use this to check how well the Neural Operator approximates the ground truth.
The Poisson equation is given by
$$
-\frac{\partial^2 u}{\partial x^2} = f(x), \quad x \in (0, 1),
$$
with boundary conditions $u(0) = u(1) = 0$.
The fundamental solution (also called Green's function) is given by
$$
k(x, y) = \frac{1}{2}(x+y - |y-x|) - xy,
$$
so the solution for any $f$ is given by
$$
u(x) = \int_0^1 k(x, y) f(y) \, dy.
$$
Note that---as pointed out previously---a fundamental solution does not exist for every PDE---only a subset of linear PDEs with appropriate boundary conditions have one.
A full Neural Operator architecture has several layers and thus, we don't usually have a direct representation of the fundamental solution, either.
This simple example of a Poisson equation was trained with a single layer, no bias and linear skip connection, and identity activation, so that the learned kernel is directly interpretable as the fundamental solution and can be compared to the ground truth.
@fig-poisson-equation shows both the analytic fundamental solution and the learned kernel.
Of note, they are not exactly the same, but qualitatively similar, which gives rise to the good generalization properties when querying the output function at arbitrary points.

::: {#fig-poisson-equation style="width: 500px; margin: auto; text-align: center;"}
![](poisson-equation.png){width=500px}

Comparison of the fundamental solution and the learned kernel for the Poisson equation.
:::


### Darcy flow: train a simple Neural Operator yourself
Darcy flow is a relatively simple PDE that describes the flow of a fluid through a porous medium (e.g., water through sand).
It is given by
$$
-\nabla \cdot (a(x) \nabla u(x)) = f(x),
$$
where $u(x)$ is the pressure of the fluid, $a(x)$ is the local permeability of the medium, and $f(x)$ is an external source term.
The boundary conditions are usually $u(x) = u_0$ on the boundary, where $u_0$ is a given value ($u_0=0$ here).

@fig-darcy-flow-dataset shows an illustrative training dataset for the Darcy flow problem including the input function, output function and positional embedding.
@fig-darcy-flow-prediction shows the prediction of the trained Neural Operator for three different input functions $a(x)$, and ground truth for comparison.
The shown examples were reproduced by code provided by the authors (see [resources](#sec-resources) for links). They were trained on a $16 \times 16$ grid, which requires minimal computation, so you can easily train them yourself on a standard cpu.

Note that this example actually uses a slightly adapted version of the training process, where both trained frequencies and the resolution of the input data are increased incrementally.
The Incremental Fourier Neural Operator (iFNO) was introduced in @George2022.

::: {#fig-darcy-flow layout-ncol=2 style="width: 600px; margin: auto; text-align: center;"}

![](darcy-flow-dataset.png){#fig-darcy-flow-dataset}

![](darcy-flow-prediction.png){#fig-darcy-flow-prediction}

Darcy flow dataset and prediction. (a) Illustrative training dataset for the Darcy flow problem including the input function, output function, and positional embedding. (b) Prediction of the trained Neural Operator (right) for three different input functions $a(x)$ (left) and ground truth (middle) for comparison.
:::



### Inverse problems: solve them much faster than classical methods
Neural Operators can also be applied to efficiently solve Bayesian inverse problems, where the goal is to recover the initial condition or coefficient functions of a PDE from noisy observations.
Here, we look at a simulation of a two-dimensional viscous, incompressible fluid flow governed by the Navier--Stokes equations in their vorticity-streamfunction formulation.
We'll skip the details of the PDE and just note the general setup: 
@fig-inverse-problem (left column) shows the initial condition at timepoint $t=0$ and the corresponding state at timepoint $t=50$.
The goal is to recover the initial condition from sampling the state at timepoint $t=50$ which is affected by noise.

For solving the inverse problem for a single state, a classical method (middle column) based on spectral methods took 18 hours to compute the solution [@Kovachki2021].

Meanwhile, a Fourier Neural Operator (FNO) for solving the inverse problem can be trained on training data pairs obtained by forward simulation using classical methods.
After training of the Neural Operator, which took 12 hours (including generation of training data), solving the inverse problem for each new single state takes only 2.5 minutes (right column), making this approach much more efficient than solving the whole inverse problem with classical methods.

::: {#fig-inverse-problem style="width: 650px; margin: auto; text-align: center;"}
![](baysian-inverse-problem.png){width=650px}

Solving an inverse problem with classical methods and Fourier Neural Operator.
Left column: Ground truth initial condition at timepoint $t=0$ (top) and the corresponding state at timepoint $t=50$ (bottom), where the sample points are shown as circles.
Middle column: Recovered initial condition from the ground truth at timepoint $t=50$ using a classical method (top) and the correponding state at $t=50$ (bottom) for comparison.
Right column: Recovered initial condition from the ground truth at timepoint $t=50$ using a Fourier Neural Operator (top) and the corresponding state at $t=50$ (bottom) for comparison.
:::



## Applications in computer vision

Neural Operators have been successfully applied to real-world problems, including weather forecasting [@Pathak2022] (see also our blog post on [FourCastNet](../FourCastNet/index.qmd)) and CO2 storage [@Wen2023].

Moreover, Neural Operator learning is not restricted to PDEs, but can also be applied to computer vision tasks: 
images can naturally be viewed as real-valued function on 2D domains, and videos add a temporal structure.
Examples of computer vision tasks that have been successfully tackled with Neural Operators include image inpainting, super-resolution, image segmentation and classification [@Chi2020;@Guibas2022;@Wei2023;@Wong2023].



## References

::: {#refs}
:::


## Resources{#sec-resources}
* Neural Operators in PyTorch [:link:](https://neuraloperator.github.io/dev/index.html)
* Darcy flow using Fourier Neural Operators in NVIDIA Modulus [:link:](https://docs.nvidia.com/deeplearning/modulus/modulus-v2209/user_guide/neural_operators/darcy_fno.html)
* Plot Darcy Flow dataset in Python [:link:](https://github.com/neuraloperator/neuraloperator/blob/main/examples/plot_darcy_flow.py)
* Train incremental Darcy Flow example with PyTorch [:link:](https://github.com/neuraloperator/neuraloperator/blob/main/examples/plot_incremental_FNO_darcy.py)
