---
title: "Neural Operators"
from: markdown+emoji
description: "An Introduction to Neural Operators"
author: "Magdalena Schneider and Michael Innerberger"
date: "2024-11-29"
categories: [AIForScience, NumericalMethods, NeuralOperators]
image: ""  # Optional
---

*Neural Operators and Fourier Neural Operator papers were presented by Michael Innerberger and Magdalena Schneider in Janelia CVML 2024-11-01, Adaptive Fourier Neural Operator and FourCastNet papers were presented by Magdalena Schneider and Kristin Branson in Janelia CVML 2024-11-08*

Neural operators are a generalization of neural networks to continuous domains, i.e. they learn mappings from a function to a function. A major advantage of neural operators is their discretization invariance, which allows for zero-shot super resolution, meaning that the output can be evaluated at an arbitrary set of query points without the need to retrain the neural operator. Originally developed for solving PDEs, neural operators have been successfully applied to a wide range of problems, including computer vision tasks. In this blog post, we will introduce the concept of neural operators, discuss different ways to represent the kernel, and present some hands-on examples.


## Brief introduction to PDEs

Partial differential equations (PDEs) are the main mathematical tool to model and simulate complex real-world phenomena.
A PDE describes how a function $u \colon D \to \mathbb{R}$ on some domain $D \subset \mathbb{R}^d$ (this could be heat in a mechanical element, pressure in the atmosphere, etc.) behaves.
In a very general form, a PDE can be written as
$$\begin{align}
\mathsf{L}_a u = f \qquad &\text{in } D,\\
u = 0 \qquad &\text{on } \partial D,
\end{align}$$
where $\mathsf{L}_a$ is a (in general nonlinear) differential operator with some parameters $a \colon D \to \mathbb{R}^m$, $f$ is a source term, and $u$ is the solution to the PDE.
Formally, we require that $a$ is in some function space $\mathcal{A}$ and $u$ is in some function space $\mathcal{U}$.
This implies that $f \in \mathcal{U}^*$, which is the dual space of $\mathcal{U}$, and $\mathsf{L} \colon \mathcal{A} \times \mathcal{U} \to \mathcal{U}^*$.

Traditionally, the types of questions we ask about PDEs are:

- Given a PDE and a fixed set of parameters $a$, what is the solution $u$ for a given source term $f$, i.e., what is the inverse of $\mathsf{L}_a \colon \mathcal{U} \to \mathcal{U}^*$?
- Given a PDE and a fixed source term $f$, what is the solution $u$ for a given set of parameters $a$, i.e., what is the inverse of $\mathsf{L}_{(\cdot)}f \colon \mathcal{A} \to \mathcal{U}^*$?
- Given a PDE and a fixed solution $u$, what are the parameters $a$ or the source term $f$ that generated this solution, i.e., solving an *inverse problem*?

To make this abstract setting more concrete, we'll consider some examples in the following sections.


### Example: Darcy flow

Consider the Darcy flow equation, which describes the flow of a fluid (e.g., water) through a two dimensional porous medium (e.g., sand) in $D = [0, 1]^2$:
$$\begin{align}
-\nabla \cdot (a \nabla u) &= f \qquad \text{in } D,\\
u &= 0 \qquad \text{on } \partial D,
\end{align}$$
where $u$ is the pressure of a fluid in a porous medium, $a$ is the local permeability of the medium, and $f$ is an external source of pressure.
In this example, $\mathcal{A} = L^\infty(D; \mathbb{R}_+)$, and $\mathcal{U} = H^1_0(D)$.

Possible questions to ask about this PDE are:

- Given a fixed distribution of sand $a$, what is the pressure of the water $u$ given some external source of pressure $f$?
- Given a fixed external source of pressure $f$, what is the pressure of the water $u$ given some distribution of sand $a$?
- Given a fixed external source of pressure $f$ and a known pressure distribution $u$, what is the distribution of sand $a$ that generated this pressure distribution?


### Example: time dependent heat equation

Consider the one dimensional time dependent heat equation, which describes the evolution of the temperature $u$ in a unit-length rod up to time $T$:
$$\begin{align}
\partial_t u - \Delta u &= f \qquad \text{in } (0, 1) \times (0, T),\\
u &= 0 \qquad \text{on } \{0, 1\} \times (0, T),\\
u &= a \qquad \text{at } t = 0,
\end{align}$$
where $a$ is the initial temperature of the rod and $f$ is an external source of heat.
In this example, the domain $D$ is the space-time cylinder $(0, 1) \times (0, T)$, $\mathcal{A} = L^\infty((0, 1); \mathbb{R}_+)$, and $\mathcal{U} = L^2((0, T); H^1_0((0,1); \mathbb{R}))$.

Possible questions to ask about this PDE are:

- Given a fixed initial temperature $a$, what is the temperature distribution $u$ at time $T$ given some external heating $f$?
- Given a fixed external heating $f$, what is the temperature distribution $u$ at time $T$ given some initial temperature $a$?
- Given a fixed external heating $f$ and a known temperature distribution $u$, what is the initial temperature $a$ that generated this temperature distribution?
- Given a fixed initial temperature $a$ and a known temperature distribution $u$, what is the external heating $f$ that generated this temperature distribution?


### Classical answers to PDE questions

Traditionally, the answers to the questions above are obtained by solving the PDE with classical numerical methods, such as finite differences, finite elements, or spectral methods; see, e.g., [Bartels, 2018](#Bartels2018).
All of these methods require knowledge of the PDE that models the physical phenomenon of interest, and they take significant resources to implement and compute the solution.

In a small set of cases, however, a given PDE might be solved analytically.
In general, this applies to linear PDEs with constant coefficients and simple geometries.
For example, the heat equation above can be solved analytically for all given $a$ and $f$, while the Darcy flow equation cannot.
In such a case, one can find a so-called fundamental solution $k$ to the PDE, which is a solution to the PDE with a Dirac delta source term:
$$\begin{align}
\mathsf{L}_a k(\cdot, y) = \delta_y \qquad &\text{in } D,\\
k = 0 \qquad &\text{on } \partial D,
\end{align}$$
for all $y \in D$, where $\delta_y$ is the Dirac delta distribution centered at $y$.

Given such a fundamental solution, one can express the solution $u$ to the PDE by convolving the source term $f$ with the fundamental solution:
$$u(x) = \int_D k(x, y) f(y) \, dy.$$
While we, again, stress that such a representation of the solution is only possible for a small set of very specific PDEs, it will serve as the basis for neural operators in the following sections.


## What are Neural Operators?

Conventional neural networks have finite-dimensional inputs and outputs, which are represented as vectors. However, many real-world problems are continuous in nature and better represented by a function. Neural operators are a generalization of neural networks that can learn to map functions to functions. They can be used to solve PDEs, inverse problems, and other tasks that involve continuous functions. Neural operators can also be applied to learn mappings between neural representations, as neural representations are function representations.

::: {#fig-neural-operator style="width: 700px; margin: auto; text-align: center;"}
![](neural-operator.png){width=700px}

Principle of neural operators: An input function $a$ is mapped to an output function $u$.
:::

### General architecture
Put simply, neural operators can be seen as a generalization of convolutional neural networks (CNNs) to continuous domains. While CNNs feature convolutions with small kernels on discrete grids, neural operators have a continuous kernel that is convolved with the whole function in each layer.
The general architecture of a neural operator is shown in @fig-no-architecture.
The input function $a(x)$ is represented by a set of query points and corresponding function values. First, $a(x)$ is lifted into a (usually higher-dimensional) space by $P$. Then, the lifted function is processed by a series of layers, each of which applies a continuous kernel to the function. The resulting function is added to a linear term $W$ and then passed through a non-linear activation function. Finally, the output function is projected to the output space by the projection $Q$, yielding the output function $u(x)$.
Note that the output function can be queried at any point in the domain, which is a key feature of neural operators. The query points of $u(x)$ are supplied together with the input function, as they need to pass through the whole network. Importantly, the query points do not have to be the same as the sampling points of the input function, or the query points used for training of the neural operator, which allows for "zero-shot super resolution".

::: {#fig-no-architecture style="width: 700px; margin: auto; text-align: center;"}
![](no-architecture.png){width=700px}

General architecture of a neural operator. The input function $a(x)$ is lifted into a (higher-dimensional) space by $P$, processed by several neural operator layers, and projected to the output space by $Q,$ yielding the output function $u(x)$. Figure adapted from [Kovachki, 2021](#Kovachki2021).
:::


### Discretization and invariance

Although neural operators are designed to learn mappings between functions on continuous domains, for numerical implementation, the continuous functions still needs to be discretized, i.e., represented by a set of query points and corresponding function values. While, in principle, the query points can be arbitrary, in practice they are often chosen to be equidistantly spaced on a grid for computational efficiency.
Hence, the learned operator is an approximation to the true operator $\widehat{G}$, operating on
$$
\widehat{G} \colon \mathbb{R}^{L \cdot m} \to \mathbb{R}^{L \cdot d} \rightarrow \mathcal{U},
$$
where $L$ is the number of query points of the input function, $d$ and $m$ are the dimensions of input function and coefficients, respectively, and $\mathcal{U}$ is the function space of the output function.
The output function $u(x)$ can be evaluated at any set of query points, which do not have to be the same as the query points of the input function, or the query points used for training. This allows for "zero-shot super resolution", i.e., the output function can be evaluated at arbitrarily high resolution without the need to retrain the neural operator at different resolutions.

Note that some other papers have claimed that neural operators are not necessarily discretization invariant (e.g. [Golland, 2023](#Golland2023)), but this is a topic of ongoing research.


::: {#fig-discretiation-invariance style="width: 500px; margin: auto; text-align: center;"}
![](discretization-invariance.png){width=500px}

Neural operators are discretization invariant. The output can be evaluated at an arbitrary set of query points, which do not have to be the same as the query points of the input function, or the query points used for training. Figure adapted from [Kovachki, 2021](#Kovachki2021).
:::




## Different ways to represent the kernel

The bottleneck of neural operators is the convolution operation, the cost of which scales with the product of input and output size.
Assuming that input and output are of the same size $J$, the complexity of the convolution is $\mathcal{O}(J^2)$, which is infeasible for most applications.
There are multiple ways to speed up the computation of the convolution based on the structure of the kernel.

### Graph Neural Operators

Instead of taking into account the full space of pairs of input and output sample points, we can limit the evaluation of the kernel to pairs of points that are close.
In practice, this can be done by constructing a graph that connects points that are within a certain distance of each other by using a nearest neighbor search.
It is then advantageous to use graph neural networks to compute the convolution operation on this graph structure and to be able to backpropagate easily through this operation.
This reduces the complexity to $\mathcal{O}(kJ)$, where $k$ is the maximal number of neighbors within a specified distance.

::: {#fig-kernel-gno style="width: 400px; margin: auto; text-align: center;"}
![](kernel-gno.png){}

Kernel representation of a GNO (adapted from [Kovachki, 2021](#Kovachki2021))
:::


### Multipole Graph Neural Operators

Graph neural operators only take nearest neighbor interactions into account.
While this might be sufficient for some applications, long-range interactions also play a crucial role in many real-world problems (e.g., celestial mechanics, or electrodynamics).
Drawing from ideas of the fast multipole method (FMM) presented in [Greengard and Rokhlin, 1987](#FMM), multi-level neural operators use a hierarchical sequence of graphs to approximate the convolution operation.
Every node in one of these graphs summarizes the information of a group of nodes in the previous hierarchy level, and every level of the hierarchy gives rise to a certain scale dependent component of the kernel.
While the details are complex, the idea is similar to enlarging the context window of a CNN by using a sequence of convolutional layers with small kernel sizes.
This reduces the complexity to $\mathcal{O}(J)$.

::: {#fig-kernel-mgno style="width: 250px; margin: auto; text-align: center;"}
![](kernel-mgno.png){}

Multi-level graph used in MGNOs (adapted from [Kovachki, 2021](#Kovachki2021))
:::


### Low-rank Neural Operators

By assuming that the kernel can be approximated by a low-rank matrix,
$$k(x, y) \approx \sum_{i=1}^r \varphi_i(x) \varphi_i(y),$$
with learnable functions $\varphi_i \colon D \to \mathbb{R}$, and $r \ll J$, the convolution operation can be computed in $\mathcal{O}(rJ)$.

::: {#fig-kernel-lno style="width: 400px; margin: auto; text-align: center;"}
![](kernel-lno.png){}

Kernel representation of a LNO (adapted from [Kovachki, 2021](#Kovachki2021))
:::


### Fourier Neural Operators

Fourier neural operators are based on the Fourier representation of the convolution operation:
$$\int_D k(x, y) v(y) \, dy = \mathcal{F}^{-1}(\mathcal{F}(k) \odot \mathcal{F}(v)),$$
where $\mathcal{F}$ is the Fourier transform, and $\odot$ denotes the element-wise product.
In practice, this seems to give the best results and can be combined with low-pass filtering to reduce the computational cost even further.
While computing the Fourier transform scales quadratically with the input size, the fast Fourier transform (FFT) can be used to compute it in almost linear time if the input grid is uniform.
In this case, the complexity of the convolution operation is $\mathcal{O}(J \log J)$.

::: {#fig-kernel-lno style="width: 400px; margin: auto; text-align: center;"}
![](kernel-fno.png){}

Kernel representation of a FNO (adapted from [Li, 2020](#Li2020))
:::


## Hands-on examples

[Kovachki, 2021](#Kovachki2021) provides various examples of solving PDEs with neural opartors. We picked three examples here that (i) compare the performance of neural operators to the analytic solution, (ii) show a simple example that you can train yourself with code provided by the authors, and (iii) demonstrate how neural operators can solve inverse problems much faster than classical methods. 

### Poisson equation --- comparison to ground truth
The Poisson equation is a fairly simple PDE that has an analytical solution. We can use this to check how well the neural operator approximates the ground truth.
The Poisson equation is given by
$$
-\frac{\partial^2 u}{\partial x^2} = f(x), \quad x \in (0, 1),
$$
with boundary conditions $u(0) = u(1) = 0$.
The fundamental solution is given by
$$
G(x, y) = \frac{1}{2}(x+y - |y-x|) - xy,
$$
so the solution for any $f$ is given by
$$
u(x) = \int_0^1 G(x, y) f(y) dy.
$$
Note that a fundamental solution does not exist for every PDE---only a subset of linear PDEs with appropriate boundary conditions have one.
A full neural operator architecture has several layers and thus, we don't usually have a direct representation of the fundamental solution, either. This simple example of a Poisson equation was trained with a single layer, so the learned kernel is directly interpretable as the fundamental solution and can be compared to the ground truth. @fig-poisson-equation shows both the analytic fundamental solution and the learned kernel. Of note, they are not exactly the same, but qualitatively similar.

::: {#fig-poisson-equation style="width: 500px; margin: auto; text-align: center;"}
![](poisson-equation.png){width=500px}

Comparison of the fundamental solution and the learned kernel for the Poisson equation.
:::



### Train a neural operator on Darcy flow yourself
Darcy flow is a relatively simple PDE that describes the flow of a fluid through a porous medium. It is given by
$$
-\nabla \cdot (a(x) \nabla u(x)) = f(x),
$$
where $u(x)$ is the pressure, $a(x)$ is the permeability, and $f(x)$ is a source term. The boundary conditions are usually $u(x) = u_0$ on the boundary, where $u_0$ is a given value ($u_0=0$ here).


Note that this example actually uses a slightly adapted version of the training process, where both trained frequencies and the resolution of the input data are increased incrementally. The Incremental Fourier Neural Operator (iFNO) was introduced in [George, 2022](#George2022).

::: {#fig-darcy-flow layout-ncol=2 style="width: 600px; margin: auto; text-align: center;"}

![](darcy-flow-dataset.png){#fig-darcy-flow-dataset}

![](darcy-flow-prediction.png){#fig-darcy-flow-prediction}

Darcy flow dataset and prediction.
:::



### Solve inverse problemes much faster than classical methods
Baysian inverse problem:
Recover initial condition or coefficient functions from noisy observations

Computation time
Classical method: 18h
FNO: 2.5min (+ once: 12h training)

::: {#fig-inverse-problem style="width: 500px; margin: auto; text-align: center;"}
![](baysian-inverse-problem.png){width=500px}

Solving an inverse problem with classical methods and Fourier Neural Operators.
:::



## Applications in computer vision

Real-world applications: weather forecasting (see other blog), CO2 storage (cite paper)

Operator learning is not restricted to PDEs

images can naturally be viewed as real-valued function on 2D domains
videos add a temporal structure

Examples:
Chi et al., Fast Fourier Convolution, 2020
Chu et al., Rethinking Fast Fourier Convolution in Image Inpainting, 2023
Wei et al., Super-Resolution Neural Operator, 2023
Wong et al., FNOSeg3D: Resolution-robust 3D image segmentation with Fourier Neural Operator




## References
<a id="Bartels2018">[Bartels, 2018]</a> 
Sören Bartels, "Numerical Approximation of Partial Differential Equations", 2018. [:link:](https://link.springer.com/book/10.1007/978-3-319-32354-1)

<a id="George2022">[George, 2022]</a> 
Robert Joseph George, Jiawei Zhao, Jean Kossaifi, Zongyi Li and Anima Anandkumar , "Incremental Spatial and Spectral Learning of Neural Operators for Solving Large-Scale PDEs", 2022. [:link:](https://doi.org/10.48550/arXiv.2211.15188)

<a id="FMM">[Greengard and Rokhlin, 1987]</a> 
Leslie Greengard and Vladimir Rokhlin, "A fast algorithm for particle simulations", 1987. [:link:](https://www.sciencedirect.com/science/article/pii/0021999187901409)

<a id="Kovachki2021">[Kovachki, 2021]</a> 
Nikola B. Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew M. Stuart and Anima Anandkumar, "Neural Operator: Learning Maps Between Function Spaces", 2021. [:link:](https://doi.org/10.48550/arXiv.2108.08481)

<a id="Li2020">[Li, 2020]</a> 
Zongyi Li, Nikola B. Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew M. Stuart and Anima Anandkumar, "Fourier Neural Operator for Parametric Partial Differential Equations", 2020. [:link:](https://doi.org/10.48550/arXiv.2010.08895)

## Resources
* Neural Operators in PyTorch [:link:](https://neuraloperator.github.io/dev/index.html)
* Darcy flow using Fourier Neural Operators in NVIDIA Modulus [:link:](https://docs.nvidia.com/deeplearning/modulus/modulus-v2209/user_guide/neural_operators/darcy_fno.html)
* Plot Darcy Flow dataset in Python [:link:](https://github.com/neuraloperator/neuraloperator/blob/main/examples/plot_darcy_flow.py)
* Train incremental Darcy Flow example with PyTorch [:link:](https://github.com/neuraloperator/neuraloperator/blob/main/examples/plot_incremental_FNO_darcy.py)
