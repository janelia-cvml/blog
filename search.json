[
  {
    "objectID": "posts/NeuralOperators/index.html",
    "href": "posts/NeuralOperators/index.html",
    "title": "Neural Operators",
    "section": "",
    "text": "Neural Operators (Kovachki et al. 2022) and Fourier Neural Operator (Li et al. 2021) papers were presented by Michael Innerberger and Magdalena Schneider in Janelia CVML 2024-11-01 (and discussed in this blog post), Adaptive Fourier Neural Operator (Guibas et al. 2022) and FourCastNet (Pathak et al. 2022) papers were presented by Magdalena Schneider and Kristin Branson in Janelia CVML 2024-11-08 (see blog post on FourCastNet)\nNeural Operators extend the concept of neural networks to work directly with functions rather than discrete data points. While neural networks typically take vectors or tensors as inputs and produce similar outputs, Neural Operators learn mappings between entire functions. A major advantage of neural operators is their (presumed) discretization invariance, which allows for zero-shot super resolution, meaning that the output function can be evaluated at an arbitrary set of query points without the need to retrain the neural operator. Originally developed for solving PDEs, neural operators have been successfully applied to a wide range of problems, including computer vision tasks. In this blog post, we will introduce the concept of neural operators, discuss different ways to represent the kernel, and present some hands-on examples.\nWe assume that you are familiar with some fundamental concepts of vector calculus like partial derivatives of multi-variate functions. Some background in Partial Differential Equations (PDEs) and fundamental solutions to PDEs would be ideal as this is the theoretical foundation of Neural Operators, but not strictly necessary."
  },
  {
    "objectID": "posts/NeuralOperators/index.html#what-are-neural-operators",
    "href": "posts/NeuralOperators/index.html#what-are-neural-operators",
    "title": "Neural Operators",
    "section": "What are Neural Operators?",
    "text": "What are Neural Operators?\nConventional neural networks have finite-dimensional inputs and outputs, which are represented as vectors. However, many real-world problems are continuous in nature and better represented by a function. Neural Operators are a generalization of neural networks that can learn to map functions to functions. They can be used to solve PDEs, inverse problems, and other tasks that involve finding functions defined over continuous domains. Neural operators can also be applied to learn mappings between neural representations, as neural representations are function representations.\n\n\n\n\n\n\nFigure 1: Principle of neural operators: An input function \\(a\\) is mapped to an output function \\(u\\).\n\n\n\n\nMotivation\nThe main motivation behind Neural Operators is that some linear PDEs of the form \\(\\mathsf{L}_a u = f\\) with parameter functions \\(a \\colon D \\to \\mathbb{R}^m\\) in some domain \\(D \\subset \\mathbb{R}^d\\) and source term \\(f\\) can be solved by convolution with a fundamental solution \\(k\\) of the PDE. Note that either the source term \\(f\\) or the parameter function \\(a\\) can be fixed, and the other is the input to the solution operator which gives us \\(u\\). If we want to find the solution for varying \\(f\\), the solution operator \\(G \\colon f \\mapsto u\\) mapping a given source term \\(f\\) to the solution \\(u\\) is given by \\[\nu(x) = \\int_D k(x, y) f(y) \\, dy.\n\\tag{1}\\] While this is only possible for a small set of very specific PDEs, this representation of the solution serves as a basic building block for neural operators, on top of which a deep learning architecture is built. We stress that Equation 1 is only a motivation and not a theoretical justification for why neural operators work. In particular, neural operators can be applied to a much wider range of problems such as non-linear PDEs and inverse problems; in particular, in the following, we consider the case where the source term \\(f\\) is fixed and the Neural Operator learns to map a parameter function \\(a\\) to the solution \\(u\\).\n\n\nIllustrative example\nTo illustrate the concept of how Neural Operators solve PDEs, we will consider a simple example of a PDE and return to this example throughout the text to explain the different concepts. The physical model for the Darcy flow is a relatively simple PDE that describes the flow of a fluid through a porous medium (e.g., water through sand). In its two-dimensional form on the unit square \\(D = [0, 1]^2\\), the Darcy flow equation is given by \\[\n-\\nabla \\cdot (a(x) \\nabla u(x)) = f(x),\n\\] where \\(u(x)\\) is the pressure of the fluid, \\(a(x)\\) is the local permeability of the medium, and \\(f(x)\\) is an external source term modeling fluid entering or leaving the system. Without porous medium, fluid flow is given by the pressure gradient \\(\\nabla u\\), so \\(a(x)\\) intuitively acts as a local modifier of the driving force behind the flow. The quantity \\(a(x) \\nabla u(x)\\) is called the flux of the fluid, and the Darcy flow equation states that the sources (which are given by the divergence operator \\(\\nabla \\cdot\\)) of the fluid flux are given by \\(f\\).\nFixing the source term \\(f(x) = 1\\) and the boundary condition \\(u(x) = 0\\) (meaning that there is no fluid pressure) on the boundary \\(\\partial D\\), the problem is to find the pressure field \\(u(x)\\) for a given permeability field \\(a(x)\\). Pairs of \\(a(x)\\) and \\(u(x)\\) are shown in Figure 9, where the input function \\(a(x)\\) is shown on the left as a binary scalar field over the unit square, and the corresponding output function \\(u(x)\\) is shown on the right, having a more intricate structure. Note that the images shown in the figure are just discrete representations of the functions \\(a(x)\\) and \\(u(x)\\) defined on the whole domain \\(D\\), which are the input and output of the PDE problem (and the Neural Operator).\nIn the context of Neural Operators, the goal is to learn the inverse \\(\\mathsf{L}_a^{-1}f\\) of the differential operator \\(\\mathsf{L}_a = -\\nabla \\cdot (a(x) \\nabla (\\cdot))\\), which maps the parameter function \\(a(x)\\) to the solution \\(u(x)\\). Note that the Darcy flow with a general permeability field \\(a(x)\\) doesn’t have an analytically known fundamental solution, and the mapping from \\(a(x)\\) to \\(u(x)\\) is highly non-linear, so that in order to solve the PDE numerical or machine learning methods are necessary, such as Neural Operators.\n\n\nGeneral architecture\nPut simply, Neural Operators can be seen as a generalization of convolutional neural networks (CNNs) to continuous domains. While CNNs feature convolutions with small kernels on regular discrete grids, neural operators have a continuous kernel that is convolved with the whole function in each layer. The general architecture of a Neural Operator is shown in Figure 2. The input function \\(a\\) (usually represented by a set of query points and corresponding function values—see Section 1.4) is lifted into a (usually higher-dimensional) space by \\(P\\). Then, the lifted function is processed by a series of layers, each of which applies a convolution with a continuous kernel to the function. The resulting function is added to a linear term \\(W v\\) and then passed through a non-linear activation function; note that this is akin to a skip connection in a CNN. Finally, the result of this computation is projected to the output space by the projection \\(Q\\), yielding the output function \\(u\\). Note that the output function can be queried at any point in the domain, which is a key feature of Neural Operators.\n\n\n\n\n\n\nFigure 2: General architecture of a Neural Operator. The input function \\(a(x)\\) is lifted into a (higher-dimensional) space by \\(P\\), processed by several Neural Operator layers, and projected to the output space by \\(Q,\\) yielding the output function \\(u(x)\\). Figure adapted from Kovachki et al. (2022).\n\n\n\n\n\nDiscretization and invariance\nAlthough Neural Operators are designed to learn mappings between functions on continuous domains, for numerical implementation, the functions still needs to be discretized, i.e., represented by a set of query points and corresponding function values \\(\\{x_i, a(x_i)\\}_i\\). The query points of \\(u(x)\\) are supplied together with the input function, as they need to pass through the whole network. While, in principle, the query points can be arbitrary, in practice they are often chosen to be equidistantly spaced on a grid for computational efficiency. Hence, the true learned operator \\(\\widehat{G} \\colon a \\mapsto u\\) (operating on functions) is approximated by the discretized operator \\(\\widehat{G}_L\\), operating on \\[\n\\widehat{G}_L \\colon \\mathbb{R}^{L \\cdot m} \\times \\mathbb{R}^{L \\cdot d} \\rightarrow \\mathcal{U},\n\\] where \\(L\\) is the number of query points of the input function, \\(d\\) and \\(m\\) are the dimensions of the domain \\(D\\) and the image of the input function, respectively, and \\(\\mathcal{U}\\) is the function space of the output function. Notably, the output function \\(u(x)\\) can be evaluated at any set of query points, which do not have to be the same as the query points of the input function, or the query points used for training. This allows for “zero-shot super resolution”, i.e., the output function can be evaluated at arbitrarily high resolution without the need to retrain the neural operator at different resolutions.\nDiscretization invariance refers to the fact that \\(\\widehat{G}_L\\) converges to \\(\\widehat{G}\\) as \\(L \\to \\infty\\) (under some mild conditions to the distribution of the query points). Again, increasing \\(L\\) does not require retraining the Neural Operator. Note that some other papers have claimed that Neural Operators are not necessarily discretization invariant (e.g., Wang and Golland (2023)); this is a topic of ongoing research.\n\n\n\n\n\n\nFigure 3: Neural Operators are discretization invariant. The output can be evaluated at an arbitrary set of query points, which do not have to be the same as the query points of the input function, or the query points used for training. Figure adapted from Kovachki et al. (2022).\n\n\n\nIn our example of the Darcy flow from Section 1.2 and Figure 9, the input function \\(a(x)\\) is discretized on a grid of \\(L = 16 \\times 16 = 256\\) query points, resulting in a matrix with \\(256\\) rows of the form \\(\\{x_{i,1}, x_{i,2}, a(x_{i})\\}\\) being supplied to the Neural Operator. Each row is mapped to a positional embedding by the projection \\(P\\), and the results are used as values \\(v(y)\\) in the convolution operation. In order to evaluate the output function \\(u\\) at a point \\(z \\in D\\), the query point \\(z\\) is supplied to the Neural Operator together with point evaluations of the input function \\(a(x)\\) to obtain an output value \\(u(y)\\). Since this can be done for any point \\(z\\), the output of the Neural Operator is a proper function (meaning infinite resolution), while the input is still a set of query points and function values. If more sampling points of the input function are supplied, discretization invariance ensures that the output function becomes more accurate, converging to the true solution (where the input is the full function \\(a\\)) as the number \\(L\\) of query points goes to infinity. Notably, this doesn’t require retraining the Neural Operator."
  },
  {
    "objectID": "posts/NeuralOperators/index.html#different-ways-to-represent-the-kernel",
    "href": "posts/NeuralOperators/index.html#different-ways-to-represent-the-kernel",
    "title": "Neural Operators",
    "section": "Different ways to represent the kernel",
    "text": "Different ways to represent the kernel\nThe bottleneck of Neural Operators is the convolution operation, the cost of which scales with the product of input and output size. Assuming that input and output are queried at the same \\(L\\) points, the complexity of the convolution is \\(\\mathcal{O}(L^2)\\), which is infeasible for most applications. Fortunately, there are multiple ways to speed up the computation of the convolution based on the structure of the kernel, which we will shortly describe in the following (for details see Kovachki et al. (2022)).\n\nGraph Neural Operators\nInstead of taking into account the full space of pairs of sample points, we can limit the evaluation of the kernel to pairs of points that are close to each other. In practice, this can be done by constructing a graph that connects points that are within a certain distance of each other by using a nearest neighbor search. It is then advantageous to use graph neural networks to compute the convolution operation on this graph structure, and to be able to backpropagate easily through this operation. This reduces the complexity to \\(\\mathcal{O}(kL)\\), where \\(k\\) is the maximal number of neighbors within a specified distance. Note that \\(k\\) can, in general, not be chosen independently of \\(L\\) (the number of query points of the input function) as \\(L\\) increases.\n\n\n\n\n\n\nFigure 4: Kernel representation of a GNO (adapted from Kovachki et al. (2022))\n\n\n\n\n\nMultipole Graph Neural Operators\nGraph Neural Operators only take nearest neighbor interactions into account. While this might be sufficient for some applications, long-range interactions also play a crucial role in many real-world problems (e.g., celestial mechanics, or electrodynamics). Drawing from ideas of the fast multipole method (FMM) presented in Greengard and Rokhlin (1987), Multipole Graph Neural Operators (MGNOs) use a hierarchical sequence of graphs to approximate the convolution operation. Every node in one of these graphs summarizes the information of a group of nodes in the previous hierarchy level as shown in Figure 5, and every level of the hierarchy gives rise to a certain scale-dependent component of the kernel. While the details are complex, the idea is similar to enlarging the context window of a CNN by using a sequence of convolutional layers with small kernel sizes. This reduces the complexity to \\(\\mathcal{O}(L)\\).\n\n\n\n\n\n\nFigure 5: Multi-level graph used in MGNOs (adapted from Kovachki et al. (2022))\n\n\n\n\n\nLow-rank Neural Operators\nBy assuming that the kernel can be approximated by a low-rank factorization, \\[k(x, y) \\approx \\sum_{i=1}^r \\varphi_i(x) \\varphi_i(y),\\] with learnable functions \\(\\varphi_i \\colon D \\to \\mathbb{R}\\), and \\(r \\ll L\\), the convolution operation can be computed in \\(\\mathcal{O}(rL)\\). While this can significantly reduce the computational cost, it is important to note that the low-rank assumption might not hold for all problems and that the choice of low-rank approximation (i.e., the functions \\(\\varphi\\)) can be crucial but non-trivial.\n\n\n\n\n\n\nFigure 6: Kernel representation of a LNO (adapted from Kovachki et al. (2022))\n\n\n\n\n\nFourier Neural Operators\nFourier Neural Operators (FNOs) are based on the Fourier representation of the convolution operation: \\[\\int_D k(x, y) v(y) \\, dy = \\mathcal{F}^{-1}(\\mathcal{F}(k) \\odot \\mathcal{F}(v)),\\] where \\(\\mathcal{F}\\) is the Fourier transform, and \\(\\odot\\) denotes the element-wise vector or matrix product. In practice, this seems to give the best results and can be combined with low-pass filtering to reduce the computational cost even further; see Figure 7, where the Fourier representation \\(R\\) of the kernel is learned and only low frequencies are propagated through the network.\nWhile, in general, computing the Fourier transform scales quadratically with the input size, the fast Fourier transform (FFT) can be used to compute it in almost linear time if the input grid is uniform on a periodic domain (which by embedding the domain \\(D\\) in a larger domain and extending the input function by zero padding can be achieved for many applications). In this case, the complexity of the convolution operation is \\(\\mathcal{O}(L \\log L)\\).\n\n\n\n\n\n\nFigure 7: Kernel representation of a FNO (adapted from Li et al. (2021))"
  },
  {
    "objectID": "posts/NeuralOperators/index.html#practical-examples",
    "href": "posts/NeuralOperators/index.html#practical-examples",
    "title": "Neural Operators",
    "section": "Practical examples",
    "text": "Practical examples\nKovachki et al. (2022) provides various examples of solving PDEs with Neural Operators. We picked three examples here that (i) compare the performance of neural operators to the analytic solution, (ii) show a simple example that you can train yourself with code provided by the authors, and (iii) demonstrate how Neural Operators can solve inverse problems much faster than classical methods.\nNote that, in order to generate training data, in Kovachki et al. (2022) the PDEs were solved with classical numerical methods, such as finite differences, finite elements, or spectral methods; see, e.g., Bartels (2018). Of note, all of these methods require knowledge of the PDE that models the physical phenomenon of interest, and they take significant resources to implement and compute the solution. While using data from classical methods to train is one common approach, it is also possible to train neural operators directly from data, which was done, for example, in the FourCastNet paper (Pathak et al. 2022), and is an active area of research.\n\nPoisson equation: compare fundamental solutions\nThe Poisson equation is a fairly simple PDE that has an analytical solution. We can use this to check how well the Neural Operator approximates the ground truth. The Poisson equation is given by \\[\n-\\frac{\\partial^2 u}{\\partial x^2} = f(x), \\quad x \\in (0, 1),\n\\] with boundary conditions \\(u(0) = u(1) = 0\\). The fundamental solution (also called Green’s function) is given by \\[\nk(x, y) = \\frac{1}{2}(x+y - |y-x|) - xy,\n\\] so the solution for any \\(f\\) is given by \\[\nu(x) = \\int_0^1 k(x, y) f(y) \\, dy.\n\\] Note that—as pointed out previously—a fundamental solution does not exist for every PDE—only a subset of linear PDEs with appropriate boundary conditions have one. A full Neural Operator architecture has several layers and thus, we don’t usually have a direct representation of the fundamental solution, either. This simple example of a Poisson equation was trained with a single layer, no bias and linear skip connection, and identity activation, so that the learned kernel is directly interpretable as the fundamental solution and can be compared to the ground truth. Figure 8 shows both the analytic fundamental solution and the learned kernel. Of note, they are not exactly the same, but qualitatively similar, which gives rise to the good generalization properties when querying the output function at arbitrary points.\n\n\n\n\n\n\nFigure 8: Comparison of the fundamental solution and the learned kernel for the Poisson equation (adapted from Kovachki et al. (2022)).\n\n\n\n\n\nDarcy flow: train a simple Neural Operator yourself\nWe again turn to the Darcy flow problem from Section 1.2. Figure 9 (a) shows an illustrative training dataset for the Darcy flow problem including the input function, output function and positional embedding. Figure 9 (b) shows the prediction of the trained Neural Operator for three different input functions \\(a(x)\\), and ground truth for comparison. The shown examples were reproduced by code provided by the authors (see resources for links). They were trained on a \\(16 \\times 16\\) grid, which requires minimal computation, so you can easily train them yourself on a standard cpu.\nNote that this example actually uses a slightly adapted version of the training process, where both trained frequencies and the resolution of the input data are increased incrementally. The Incremental Fourier Neural Operator (iFNO) was introduced in George et al. (2022).\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 9: Darcy flow dataset and prediction. (a) Illustrative training dataset for the Darcy flow problem including the input function, output function, and positional embedding. (b) Prediction of the trained Neural Operator (right) for three different input functions \\(a(x)\\) (left) and ground truth (middle) for comparison. Figures recreated and adapted from resources [3] and [4].\n\n\n\n\n\nInverse problems: solve them much faster than classical methods\nNeural Operators can also be applied to efficiently solve Bayesian inverse problems, where the goal is to recover the initial condition or coefficient functions of a PDE from noisy observations. Here, we look at a simulation of a two-dimensional viscous, incompressible fluid flow governed by the Navier–Stokes equations in their vorticity-streamfunction formulation. We’ll skip the details of the PDE and just note the general setup: Figure 10 (left column) shows the initial condition at timepoint \\(t=0\\) and the corresponding state at timepoint \\(t=50\\). The goal is to recover the initial condition from sampling the state at timepoint \\(t=50\\) which is affected by noise.\nFor solving the inverse problem for a single state, a classical method (middle column) based on spectral methods took 18 hours to compute the solution (Kovachki et al. 2022).\nMeanwhile, a Fourier Neural Operator (FNO) for solving the inverse problem can be trained on training data pairs obtained by forward simulation using classical methods. After training of the Neural Operator, which took 12 hours (including generation of training data), solving the inverse problem for each new single state takes only 2.5 minutes (right column), making this approach much more efficient than solving the whole inverse problem with classical methods.\n\n\n\n\n\n\nFigure 10: Solving an inverse problem with classical methods and Fourier Neural Operator. Left column: Ground truth initial condition at timepoint \\(t=0\\) (top) and the corresponding state at timepoint \\(t=50\\) (bottom), where the sample points are shown as circles. Middle column: Recovered initial condition from the ground truth at timepoint \\(t=50\\) using a classical method (top) and the correponding state at \\(t=50\\) (bottom) for comparison. Right column: Recovered initial condition from the ground truth at timepoint \\(t=50\\) using a Fourier Neural Operator (top) and the corresponding state at \\(t=50\\) (bottom) for comparison. Figure adapted from Kovachki et al. (2022)."
  },
  {
    "objectID": "posts/NeuralOperators/index.html#applications-in-computer-vision",
    "href": "posts/NeuralOperators/index.html#applications-in-computer-vision",
    "title": "Neural Operators",
    "section": "Applications in computer vision",
    "text": "Applications in computer vision\nNeural Operators have been successfully applied to real-world problems, including weather forecasting (Pathak et al. 2022) (see also our blog post on FourCastNet) and CO2 storage (Wen et al. 2023).\nMoreover, Neural Operator learning is not restricted to PDEs, but can also be applied to computer vision tasks: images can naturally be viewed as real-valued function on 2D domains, and videos add a temporal structure. Examples of computer vision tasks that have been successfully tackled with Neural Operators include image inpainting, super-resolution, image segmentation and classification (Chi, Jiang, and Mu 2020; Guibas et al. 2022; Wei and Zhang 2023; Wong, Wang, and Syeda-Mahmood 2023)."
  },
  {
    "objectID": "posts/NeuralOperators/index.html#key-takeaways",
    "href": "posts/NeuralOperators/index.html#key-takeaways",
    "title": "Neural Operators",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nNeural Operators are a cool tool for simulating real-world problems with neural networks, combining concepts from PDE theory and known neural network architectures.\nThe performance gains and trainability from data that Neural Operators promise might help to overcome fundamental problems in biological problems. This could mean a renaissance for simulation-based approaches in biological sciences. While the highest computational effficieny is achieved in somewhat specific settings (e.g., requiring uniform grids), more research is currently being done to extend this efficiency to more general settings.\nWith all the talk about mapping functions to functions, Neural Operators are still working with discrete (though arbitrary) query points. Whether this will be a limitation in the future remains to be seen and more theoretical work is needed to understand Neural Operators better. For example, it is currently being questioned if the concept of discretization invariance holds true in general.\nThe authors provide a codebase for Neural Operators in PyTorch, so they can easily be adopted. Furthermore, there is a growing community around Neural Operators, extending the concepts and implementations to new problems such as computer vision tasks (with close links to transformers)."
  },
  {
    "objectID": "posts/NeuralOperators/index.html#references",
    "href": "posts/NeuralOperators/index.html#references",
    "title": "Neural Operators",
    "section": "References",
    "text": "References\n\n\nBartels, Sören. 2018. Numerical Approximation of Partial Differential Equations. Cham, Switzerland: Springer. https://doi.org/10.1007/978-3-319-32354-1.\n\n\nChi, Lu, Borui Jiang, and Yadong Mu. 2020. “Fast Fourier Convolution.” In Advances in Neural Information Processing Systems, 33:4479–88. https://proceedings.neurips.cc/paper/2020/file/2fd5d41ec6cfab47e32164d5624269b1-Paper.pdf.\n\n\nGeorge, Robert Joseph, Jiawei Zhao, Jean Kossaifi, Zongyi Li, and Anima Anandkumar. 2022. “Incremental Spatial and Spectral Learning of Neural Operators for Solving Large-Scale PDEs.” arXiv Preprint arXiv:2211.15188. https://arxiv.org/abs/2211.15188.\n\n\nGreengard, Leslie, and Vladimir Rokhlin. 1987. “A Fast Algorithm for Particle Simulations.” Journal of Computational Physics 73 (2): 325–48. https://doi.org/10.1016/0021-9991(87)90140-9.\n\n\nGuibas, John, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro. 2022. “Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers.” In International Conference on Learning Representations. https://arxiv.org/abs/2111.13587.\n\n\nKovachki, Nikola B., Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew M. Stuart, and Anima Anandkumar. 2022. “Neural Operator: Learning Maps Between Function Spaces.” Journal of Machine Learning Research 23 (182): 1–63. https://arxiv.org/abs/2108.08481.\n\n\nLi, Zongyi, Nikola B. Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew M. Stuart, and Anima Anandkumar. 2021. “Fourier Neural Operator for Parametric Partial Differential Equations.” In International Conference on Learning Representations. https://arxiv.org/abs/2010.08895.\n\n\nPathak, Jaideep, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, et al. 2022. “FourCastNet: A Global Data-Driven High-Resolution Weather Model Using Adaptive Fourier Neural Operators.” In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, 1–12. https://doi.org/10.1109/SC41405.2022.00013.\n\n\nWang, Clinton J., and Polina Golland. 2023. “Discretization Invariant Networks for Learning Maps Between Neural Fields.” arXiv Preprint arXiv:2206.01178. https://arxiv.org/abs/2206.01178.\n\n\nWei, Min, and Xuesong Zhang. 2023. “Super-Resolution Neural Operator.” arXiv Preprint arXiv:2303.02584. https://arxiv.org/abs/2303.02584.\n\n\nWen, Gege, Zongyi Li, Qirui Long, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M. Benson. 2023. “Real-Time High-Resolution CO\\(_2\\) Geological Storage Prediction Using Nested Fourier Neural Operators.” Energy and AI 12: 100199. https://doi.org/10.1016/j.egyai.2022.100199.\n\n\nWong, Ken C. L., Hongzhi Wang, and Tanveer Syeda-Mahmood. 2023. “FNOSeg3D: Resolution-Robust 3D Image Segmentation with Fourier Neural Operator.” In Proceedings of the IEEE 20th International Symposium on Biomedical Imaging (ISBI), 1–5. https://doi.org/10.1109/ISBI53787.2023.10230586."
  },
  {
    "objectID": "posts/NeuralOperators/index.html#sec-resources",
    "href": "posts/NeuralOperators/index.html#sec-resources",
    "title": "Neural Operators",
    "section": "Resources",
    "text": "Resources\n[1] Neural Operators in PyTorch 🔗\n[2] Darcy flow using Fourier Neural Operators in NVIDIA Modulus 🔗\n[3] Plot Darcy Flow dataset in Python 🔗\n[4] Train incremental Darcy Flow example with PyTorch 🔗"
  },
  {
    "objectID": "posts/2024-11-04-welcome.html",
    "href": "posts/2024-11-04-welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "This page will highlight some papers recommended by Janelia’s computer vision and machine learning researchers."
  },
  {
    "objectID": "posts/2024-11-04-welcome.html#welcome-to-the-cvml-blog",
    "href": "posts/2024-11-04-welcome.html#welcome-to-the-cvml-blog",
    "title": "Welcome",
    "section": "",
    "text": "This page will highlight some papers recommended by Janelia’s computer vision and machine learning researchers."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Computer Vision and Machine Learning at Janelia",
    "section": "",
    "text": "Researchers at Janelia have been participating in a weekly journal club on computer vision and machine learning (CVML) since 2014. This blog began in November 2024, and will highlight some of our favorite papers.\nThis awesome logo was created by Diane Adjavon."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Janelia CVML",
    "section": "",
    "text": "Neural Operators\n\n\n\n\n\n\nAIForScience\n\n\nPartial Differential Equations\n\n\nNumerical Methods\n\n\nNeuralOperators\n\n\n\nAn Introduction to Neural Networks Operating on Function Spaces\n\n\n\n\n\nNov 29, 2024\n\n\nMagdalena Schneider and Michael Innerberger\n\n\n\n\n\n\n\n\n\n\n\n\nFourCastNet\n\n\n\n\n\n\nAIForScience\n\n\nNumericalMethods\n\n\nTransformers\n\n\nNeuralOperators\n\n\nWeather\n\n\nForecasting\n\n\n\nWeather forecasting with Adaptive Fourier Neural Operators\n\n\n\n\n\nNov 8, 2024\n\n\nKristin Branson and Magdalena Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2024\n\n\nCVML organizers\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/FourCastNet/index.html",
    "href": "posts/FourCastNet/index.html",
    "title": "FourCastNet",
    "section": "",
    "text": "Neural Operators and Fourier Neural Operator papers were presented by Michael Innerberger and Magdalena Schneider in Janelia CVML 2024-11-01 (see blog post on Neural Operators), Adaptive Fourier Neural Operator and FourCastNet papers were presented by Magdalena Schneider and Kristin Branson in Janelia CVML 2024-11-08 (and discussed in this blog post)\nTo date, weather forecasts are based on numerical weather prediction: numerical integration of partial differential equations (PDEs) describing atmospheric motion from measured initial conditions. The equations governing weather have been researched since the early 1900s [Lynch, 2008], and numerically solving these PDEs was an early success of computers [Charney, 1952]. Steady improvements in forecasting accuracy have been made, including improved data, data assimilation, and numerical methods (Figure 1).\nThere’s a pretty remarkable dataset available in the realm of AI for science: the ERA5 global reanalysis dataset, which contains hourly estimates of numerous 3D atmospheric, land, and ocean features at a horizontal resolution of 30 km over the past 80 years. This dataset was synthesized from up to 25M-per-day measurements from Earth-observing satellites and weather stations. Can machine learning be used to learn to forecast the weather better, either by more efficiently/effectively solving the weather PDEs or by learning a better model?\nThe FourCastNet paper [Pathak, 2022] trains an Adaptive Fourier Neural Operator (AFNO) network [Guibas, 2022] to predict a collection of atmospheric variables at the next time step (6 hours into the future) given the current readout for those variables. The AFNO is an interesting choice here: it takes inspiration from both neural operators, which were originally designed to efficiently solve PDEs, and Vision Transformers (ViT), which can learn complex functions from large image datasets. This choice seems to have been motivated by several factors: (i) We know that weather is well-modeled by PDEs, (ii) the ERA5 dataset can be represented as images where each pixel location corresponds to a \\(.25^\\circ \\times .25^\\circ\\) latitude/longitude region, and each channel corresponds to a different atmospheric variable. So, maybe some amalgam of neural operators and transformers is a feasible choice to capture the nature of this data. Moreover, AFNO promises to combine computational efficiency (i.e., low computational complexity) with a comparably low memory footprint."
  },
  {
    "objectID": "posts/FourCastNet/index.html#training",
    "href": "posts/FourCastNet/index.html#training",
    "title": "FourCastNet",
    "section": "Training",
    "text": "Training\nOne of the issues with almost every temporal process is that it has multiple time scales that we want to model. To predict the weather \\(k\\) time points into the future, FourCastNet must be run iteratively \\(k\\) times, and errors can accumulate so that inputs are out-of-domain. So, instead of training on the full temporal resolution of the data, FourCastNet is trained at 6-hour resolution. Probably badness would happen if it was trained at 1-hour resolution, and then run iteratively \\(6\\cdot k\\) times. Interestingly, they first train on 6-hour predictions, then fine-tune to minimize 12-hour prediction errors when calling the network twice recursively."
  },
  {
    "objectID": "posts/FourCastNet/index.html#evaluation",
    "href": "posts/FourCastNet/index.html#evaluation",
    "title": "FourCastNet",
    "section": "Evaluation",
    "text": "Evaluation\nMost of the paper is devoted to evaluating the performance of the weather forecasts. They show that their predictions are qualitatively accurate on predicting the formation and trajectory of a cyclone and hurricane, as well as patterns of an atmospheric river within the range of 2–4 days. They quantitatively compare the accuracy of the FourCastNet prediction to the ECMWF’s physics-based Integrated Forecasting System (IFS). Accuracy is worse (with a few exceptions), but comparable. For comparison, more recent ML-based models, including GraphCast [Lam, 2023] and GenCast [Price, 2024] report superior performance to ECMWF.\n\n\n\n\n\n\nFigure 3: Forecasting the formation of major storms [Pathak, 2022]\n\n\n\nThe improvement stressed in this work is computational, particularly when predicting an ensemble of forecasts. Here, they add Gaussian noise to the initial observations and produce slightly different forecasts to estimate the distribution. While the IFS forecast takes minutes on a giant supercomputing cluster, FourCastNet’s forecast takes 7 seconds on 4 A100s. There’s a bit of apples to oranges in this comparison, both in terms of the problem and the type of computers when reporting just how much more efficient their approach is, but it is unquestionably more efficient. Computational efficiency is also the major improvement that neural operators offer over classical methods for solving PDEs."
  },
  {
    "objectID": "posts/FourCastNet/index.html#thoughts",
    "href": "posts/FourCastNet/index.html#thoughts",
    "title": "FourCastNet",
    "section": "Thoughts",
    "text": "Thoughts\nOne closing question is how ML solves this problem: Is it learning to model the weather in the same way that the governing physics equations do? Or is it more about learning what initial conditions are similar and memorizing? In some ways, memorization seems like it may be a step backwards—before numerical weather prediction was a thing, meteorologists did something closer to memorization:\n\nRichardson’s book opens with a discussion of then-current practice in the Meteorological Office. He describes the use of an Index of Weather Maps, constructed by classifying old synoptic charts into categories. The Index assisted the forecaster to find previous maps resembling the current one and therewith to deduce the likely development by studying the evolution of these earlier cases. But Richardson was not optimistic about this method. He wrote that “The forecast is based on the supposition that what the atmosphere did then, it will do again now. […] The past history of the atmosphere is used, so to speak, as a full-scale working model of its present self”. […] the Nautical Almanac, that marvel of accurate forecasting, is not based on the principle that astronomical history repeats itself in the aggregate. It would be safe to say that a particular disposition of stars, planets and satellites never occurs twice. Why then should we expect a present weather map to be exactly represented in a catalogue of past weather?\n\n— From [Lynch, 2008]\nThings we are taking away from this paper:\n\nERA 5 is a cool dataset!\nIt’s going to allow ML to beat physics at weather forecasting, with lots of caveats about the classical work that goes into assimilating raw observations into these nicely gridded measurements, and probably lots of things we don’t understand about weather. Will there be noticeable improvements to us in our weather forecasts soon??\nThere’s a lot of work to do on the details of what data is used, what is predicted, and how it is evaluated. We were happy to see that the ECMWF now has an experimental AIFS—the Artificial Intelligence/Integrated Forecasting System. They’ve released implementations of several of the SOTA models that you can get with pip install ai-models\nAFNO is an interesting choice because of (a) its connection to PDEs that underly many processes we want to model and (b) computational efficiency. There are also open-source implementations of AFNO and FourCastNet, and a FourCastNet colab notebook"
  },
  {
    "objectID": "posts/FourCastNet/index.html#about-the-afno",
    "href": "posts/FourCastNet/index.html#about-the-afno",
    "title": "FourCastNet",
    "section": "About the AFNO",
    "text": "About the AFNO\nWe spent some time trying to understand the AFNO, and its relationship to both neural operators and vision transformers. A stated goal of the AFNO is to capture important properties of a transformer, but reduce the memory and computational requirements to not depend on the context length / number of image patches squared. This is important in the FourCastNet context, as each weather image each image is divided into \\(h\\times w = 720 \\times 1440\\) patches of size \\(8\\times 8\\), i.e., each image has \\(720/8 \\cdot 1440/8 = 16,200\\) patches.\nEach AFNO block does the following:\n\nLet \\(X \\in \\mathcal{R}^{h \\times w \\times d}\\) be the input, where \\(N=hw\\) is the length of the token sequence, and \\(d\\) is the token embedding dimension.\nUse the patch location \\((m,n)\\) to convert to frequency space: \\[z_{uv} = [\\textrm{DFT}(X)]_{uv} = \\frac{1}{\\sqrt{mn}} \\sum_{mn} x_{mn} \\exp( - 2\\pi i (um/h+vn/w) ).\\] This can be viewed as token mixing, creating direct connections between every input spatial token \\(x_{mn}\\) and frequency token \\(z_{uv}\\).\nIn the frequency space, channels are adaptively mixed through 2-layer MLPs: \\[\\tilde{z}_{uv} = \\text{BlockMLP}(z_{uv}) = W_2 \\sigma( W_1 z_{uv} )\\] where \\(W_1, W_2 \\in \\mathcal{R}^{d \\times d}\\) are learned block-diagonal matrices. Note that this is described as part of the “spatial mixing” of AFNO, even though, as far as we can tell, it only mixes across channels.\nAs natural images are inherently sparse in the frequency domain, sparsity can be encouraged by applying \\[S_{\\lambda}(\\tilde{z}_{uv}) := \\textrm{sign}(\\tilde{z}_{uv})\\max(|\\tilde{z}_{uv}|-\\lambda,0),\\] where the parameter \\(\\lambda\\) controls the sparsity.\nDemix tokens with the inverse Fourier transform: \\[y_{mn} = [\\textrm{IDFT}(\\tilde{Z})]_{mn} = \\frac{1}{\\sqrt{mn}} \\sum_{uv} \\tilde{z}_{uv} \\exp( 2 \\pi i (um/h+vn/w) )\\]\nPerform another two-layer MLP to mix across channels in the spatial domain: \\[\\tilde{y}_{mn} = \\text{BlockMLP}(y_{mn})\\] This step is not mentioned, as far as we can tell, in the text of the AFNO paper, but is in Figure 2 and the AFNO code base. It is referred to as channel mixing.\n\nLike the MLP mixer, its relationship to the transformer is most obviously that there is some mixing across tokens and mixing across channels. As we understand it, mixing across tokens happens through the Fourier transforms, and thus is not learned or input-dependent, but there are direct connections between all tokens.\nThe AFNO paper also shows that a self-attention layer \\[\n\\textrm{Att}(X) = \\text{softmax}( XW_q(X W_k)^\\top / \\sqrt{d} ) X W_v\n\\] can (ish) be expressed as a kernel integral: \\[\n\\textrm{Att}(X)[s] = \\sum_t X[t] \\kappa(s,t),\n\\] with \\[\\kappa(s,t) = \\text{softmax}( XW_q(X W_k)^\\top / \\sqrt{d} ) W_v\\,.\\] Note that here \\(\\kappa\\) is a kernel that depends on \\(X[t]\\). (Also note that \\(X[t]\\) is a notation convenience, assigning a linear index to each token \\(x_{mn}\\) with \\(s,t \\in [hw]\\), i.e., \\(X[t] := X[n_t,m_t]\\)). Viewing \\(X\\) as a function on the continuous space \\(D \\subset \\mathcal{R}^2\\) instead of as a matrix with values at discrete indices, they write this sum as an integral: \\[\\mathcal{K}(X)(s) = \\int_D \\kappa(s,t) X(t).\\] If we now let \\(\\kappa\\) depend only on the distance between \\(s\\) and \\(t\\), i.e., \\(\\kappa(s,t) = \\kappa(s-t)\\), then it becomes shift-invariant (which is not generally the case), and we can compute the convolution integral as multiplication in the Fourier domain: \\[\\mathcal{K}(X)(s) = \\mathcal{F}^{-1}( \\mathcal{F}(\\kappa) \\cdot \\mathcal{F}(X))(s).\\] There is a somewhat non-straightforward relationship between these equations and the steps of the AFNO. In AFNO, the multiplication is realized by the 2-layer MLP \\(\\tilde{z}_{uv} = W_2 \\sigma( W_1 z_{uv} )\\). If one wants to write this as multiplication with the Fourier transform \\(g\\) of a kernel, this becomes \\(\\tilde{z}_{uv} = W_2 \\sigma( W_1 z_{uv} ) =: g(z_{uv}) z_{uv}\\), where the kernel depends on \\(z_{uv}\\) and, hence, on \\(X\\). The reduction in complexity compared to transformers comes from the fact that the weights \\(W_1, W_2\\) here are shared for all tokens."
  },
  {
    "objectID": "posts/FourCastNet/index.html#references",
    "href": "posts/FourCastNet/index.html#references",
    "title": "FourCastNet",
    "section": "References",
    "text": "References\n[Pathak, 2022] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, Pedram Hassanzadeh, Karthik Kashinath, and Animashree Anandkumar, “FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators”, 2022. 🔗\n[Lynch, 2008] Peter Lynch, “The origins of computer weather prediction and climate modeling”, 2008. 🔗\n[Charney, 1952] J. G. Charney, R. Fjortoft, & J. Von Neumann, “Numerical integration of the barotropic vorticity equation”, 1952. 🔗\n[Haiden, 2021] T. Haiden, M. Janousek, F. Vitart, Z. Ben Bouallegue, L. Ferranti, F. Prates, and D. Richardson, “Evaluation of ECMWF forecasts, including the 2021 upgrade”, 2021. 🔗\n[Guibas, 2022] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, Bryan Catanzaro, “Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers”, 2022. 🔗\n[Lam, 2023] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, and Peter Battaglia, “GraphCast: Learning skillful medium-range global weather forecasting”, 2023. 🔗\n[Price, 2024] Ilan Price, Alvaro Sanchez-Gonzalez, Ferran Alet, Tom R. Andersson, Andrew El-Kadi, Dominic Masters, Timo Ewalds, Jacklynn Stott, Shakir Mohamed, Peter Battaglia, Remi Lam and Matthew Willson, “GenCast: Diffusion-based ensemble forecasting for medium-range weather”, 2024. 🔗"
  },
  {
    "objectID": "posts/FourCastNet/index.html#resources",
    "href": "posts/FourCastNet/index.html#resources",
    "title": "FourCastNet",
    "section": "Resources",
    "text": "Resources\n\nERA 5 dataset 🔗\nImplementations of SOTA weather forecasting models from ai-models package 🔗\nAFNO Github repo 🔗\nFourCastNet Github repo 🔗\nFourCastNet Colab notebook 🔗"
  }
]